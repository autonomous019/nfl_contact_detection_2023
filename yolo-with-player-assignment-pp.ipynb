{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install ../input/nfl-lib/timm-0.1.26-py3-none-any.whl\n!tar xfz ../input/nfl-lib/pkgs.tgz\n# for pytorch1.6\n# cmd = \"sed -i -e 's/ \\/ / \\/\\/ /' timm-efficientdet-pytorch/effdet/bench.py\"\n# !$cmd","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom glob import glob\nimport pandas as pd\nimport gc\nimport warnings\nimport shutil as sh\nfrom tqdm.auto import tqdm\nwarnings.filterwarnings(\"ignore\")","metadata":{"incorrectly_encoded_metadata":"_kg_hide-input=true _kg_hide-output=true","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nd = pd.read_csv('../input/nfl-impact-detection/test_player_tracking.csv')\nIS_PRIVATE = d.shape != (19269, 12)\nIS_PRIVATE = True\n# print(IS_PRIVATE)\n\n# IS_PRIVATE = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_ROOT_PATH = 'test_images'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mk_images(video_name, video_labels, video_dir, out_dir, only_with_impact=True):\n    video_path=f\"{video_dir}/{video_name}\"\n    video_name = os.path.basename(video_path)\n    vidcap = cv2.VideoCapture(video_path)\n    if only_with_impact:\n        boxes_all = video_labels.query(\"video == @video_name\")\n        print(video_path, boxes_all[boxes_all.impact > 1.0].shape[0])\n    else:\n        print(video_path)\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        frame += 1\n        if only_with_impact:\n            boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n            boxes_with_impact = boxes[boxes.impact > 1.0]\n            if boxes_with_impact.shape[0] == 0:\n                continue\n        img_name = f\"{video_name}_frame{frame}\"\n        image_path = f'{out_dir}/{video_name}'.replace('.mp4',f'_{frame}.png')\n        _ = cv2.imwrite(image_path, img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from math import sqrt\nfrom joblib import Parallel, delayed\nfrom tqdm.auto import tqdm\nif IS_PRIVATE:\n    out_dir = DATA_ROOT_PATH\n    if not os.path.exists(out_dir) or True:\n        !mkdir -p $out_dir\n        video_dir = '/kaggle/input/nfl-impact-detection/test'\n        uniq_video = [path.split('/')[-1] for path in glob(f'{video_dir}/*.mp4')]\n        Parallel(n_jobs=4)(delayed(mk_images)(video_name, pd.DataFrame(), video_dir, out_dir, only_with_impact=False) for video_name in tqdm(uniq_video))\n\n#         for video_name in uniq_video:\n#             mk_images(video_name, pd.DataFrame(), video_dir, out_dir, only_with_impact=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r ../input/my-yolov5-train/yolov5/* .\n# !cp -r ../input/yolov5/utils/* ./utils/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import argparse\nfrom utils.datasets import *\nfrom utils.general import non_max_suppression\nfrom utils.general import *\nfrom utils import torch_utils \nfrom utils.plots import plot_one_box","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nnames = ['0','1','2']\ndef detect(save_img=False):\n    weights, imgsz = opt.weights,opt.img_size\n    source = 'test_images/'\n    \n    # Initialize\n    device = torch_utils.select_device(opt.device)\n    half = False\n    # Load model\n\n    model = torch.load(weights, map_location=device)['model'].to(device).float().eval()\n\n    dataset = LoadImages(source, img_size=opt.img_size)\n\n    t0 = time.time()\n    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n    all_path=[]\n    all_bboxex =[]\n    all_score =[]\n    all_c = []\n    for path, img, im0s, vid_cap in tqdm(dataset):\n        print(im0s.shape)\n#         if path.split(\"/\")[-1]==\"57596_002686_Endzone_426.jpg\":\n#             break\n        img = torch.from_numpy(img).to(device)\n        img = img.float()  # uint8 to fp16/32\n        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n        if img.ndimension() == 3:\n            img = img.unsqueeze(0)\n\n        # Inference\n        t1 = torch_utils.time_synchronized()\n        bboxes_2 = []\n        score_2 = []\n        c_2 = []\n        if True:\n            pred = model(img, augment=opt.augment)[0]\n            pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=None, agnostic=False)\n            t2 = torch_utils.time_synchronized()\n            flag=False\n            bboxes = []\n            score = []\n            cc = []\n            # Process detections\n            for i, det in enumerate(pred):  # detections per image\n                p, s, im0 = path, '', im0s\n                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  #  normalization gain whwh\n                if det is not None and len(det):\n                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n                    for c in det[:, -1].unique():\n                        n = (det[:, -1] == c).sum()  # detections per class\n\n#                     for *xyxy, conf, cls in reversed(det):\n#                         if cls==2:\n#                             flag = True\n#                             label = '%s %.2f' % (names[int(cls)], conf)\n#                             plot_one_box(xyxy, im0, label=label, color=255, line_thickness=3)\n#             if flag:\n#                 plt.imshow(im0)\n#                 plt.show()\n                    for *xyxy, conf, cls in det:\n#                         if cls==2 or cls=='2':  # Write to file\n                            xywh = torch.tensor(xyxy).view(-1).numpy()  # normalized xywh\n#                             xywh[2] = xywh[2]-xywh[0]\n#                             xywh[3] = xywh[3]-xywh[1]\n                            bboxes.append(xywh)\n                            score.append(conf.cpu().numpy())\n                            cc.append(cls.cpu().numpy())\n            bboxes_2.append(bboxes)\n            score_2.append(score)\n            c_2.append(cc)\n        all_path.append(path)\n        all_score.append(score_2)\n        all_bboxex.append(bboxes_2)\n        all_c.append(c_2)\n    return all_path,all_score,all_bboxex,all_c\n\n\nif __name__ == '__main__':\n    class opt:\n        weights = \"../input/yolov5temp1280/best (2).pt\"\n        img_size = 1280\n        conf_thres = 0.1\n        iou_thres = 0.3\n        augment = False\n        device = '0'\n        classes=None\n        agnostic_nms = True\n        \n    opt.img_size = check_img_size(opt.img_size)\n    print(opt)\n\n    with torch.no_grad():\n        res = detect()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nall_path,all_score,all_bboxex,all_labels = res\n\nresults =[]\nresults_boxes =[]\nresults_scores = []\nresults_labels = []\nresult_image_ids =[]\nfor row in range(len(all_path)):\n    image_id = all_path[row].split(\"/\")[-1]\n    boxes = np.array(all_bboxex[row])[0]\n    scores = np.array(all_score[row])[0]\n    labels = np.array(all_labels[row])[0]\n    if len(boxes)==0:\n        continue\n    boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n    boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n    boxes = boxes.astype(int)\n    if len(boxes)>0:\n        result_image_ids += [image_id]*len(boxes)\n        results_boxes.append(boxes)\n        results_scores.append(scores)\n        results_labels.append(labels)\n        idx = row\n        \n\nidx = 0\nsize = 300\nidx =-1\nfont = cv2.FONT_HERSHEY_SIMPLEX \nimage = image = cv2.imread(all_path[idx], cv2.IMREAD_COLOR)\n# fontScale \nfontScale = 1\nboxes = results_boxes[idx]\nscores = results_scores[idx]\nlabel = results_labels[idx]\n# Blue color in BGR \ncolor = (255, 0, 0) \n\n# Line thickness of 2 px \nthickness = 2\nfor b,s,l in zip(boxes,scores,label):\n    color = (255,0,0) if int(l)==2 else (0,0,255)\n    image = cv2.rectangle(image, (b[0],b[1]), (b[0]+b[2],b[1]+b[3]), color, 1) \n    image = cv2.putText(image, '{:.2}'.format(s), (int(b[0]),int(b[1])), font,  fontScale, color, thickness, cv2.LINE_AA)\nplt.figure(figsize=[20,20])\nplt.imshow(image[:,:,::-1])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"box_df = pd.DataFrame(np.concatenate(results_boxes), columns=['left', 'top', 'width', 'height'])\nscore_df = pd.DataFrame({'scores':np.concatenate(results_scores),'labels':np.concatenate(results_labels), 'image_name':result_image_ids})\ntest_df = pd.concat([box_df, score_df], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#gameKey,playID,view,video,frame,left,width,top,height\n#57590,3607,Endzone,57590_003607_Endzone.mp4,1,1,1,1,1\ntest_df['gameKey'] = test_df.image_name.str.split('_').str[0].astype(int)\ntest_df['playID'] = test_df.image_name.str.split('_').str[1].astype(int)\ntest_df['view'] = test_df.image_name.str.split('_').str[2]\ntest_df['frame'] = test_df.image_name.str.split('_').str[3].str.replace('.png','').astype(int)\ntest_df['video'] = test_df.image_name.str.rsplit('_',1).str[0] + '.mp4'\n# test_df = test_df[[\"gameKey\",\"playID\",\"view\",\"video\",\"frame\",\"left\",\"width\",\"top\",\"height\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clearing working dir\n# be careful when running this code on local environment!\n# !rm -rf *\n!mv * /tmp/","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.to_csv('off.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom scipy.optimize import linear_sum_assignment","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 42\nFOLDS = 5\nFOLD = 2\nEFF_DET = 5 \nThreshold = 0.45\nLOOK_BACKWARD_LIMIT = 10\nIOU_THRESHOLD = 0.4\nIMAGE_SIZE = 512\nNUM_CLASSES = 2\nBATCH_SIZE = 16\nFRAME_THRESHOLD = 120\nkeep_columns_threshold = 20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def iou(bbox1, bbox2):\n    bbox1 = [float(x) for x in bbox1]\n    bbox2 = [float(x) for x in bbox2]\n\n    (x0_1, y0_1, x1_1, y1_1) = bbox1\n    (x0_2, y0_2, x1_2, y1_2) = bbox2\n\n    # get the overlap rectangle\n    overlap_x0 = max(x0_1, x0_2)\n    overlap_y0 = max(y0_1, y0_2)\n    overlap_x1 = min(x1_1, x1_2)\n    overlap_y1 = min(y1_1, y1_2)\n\n    # check if there is an overlap\n    if overlap_x1 - overlap_x0 <= 0 or overlap_y1 - overlap_y0 <= 0:\n            return 0\n\n    # if yes, calculate the ratio of the overlap to each ROI size and the unified size\n    size_1 = (x1_1 - x0_1) * (y1_1 - y0_1)\n    size_2 = (x1_2 - x0_2) * (y1_2 - y0_2)\n    size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0)\n    size_union = size_1 + size_2 - size_intersection\n\n    return size_intersection / size_union\n\ndef precision_calc(gt_boxes, pred_boxes):\n    cost_matix = np.ones((len(gt_boxes), len(pred_boxes)))\n    for i, box1 in enumerate(gt_boxes):\n        for j, box2 in enumerate(pred_boxes):\n            dist = abs(box1[0]-box2[0])\n            if dist > 4:\n                continue\n            iou_score = iou(box1[1:], box2[1:])\n\n            if iou_score < 0.35:\n                continue\n            else:\n                cost_matix[i,j]=0\n    row_ind, col_ind = linear_sum_assignment(cost_matix)\n    fn = len(gt_boxes) - row_ind.shape[0]\n    fp = len(pred_boxes) - col_ind.shape[0]\n    tp=0\n    for i, j in zip(row_ind, col_ind):\n        if cost_matix[i,j]==0:\n            tp+=1\n        else:\n            fp+=1\n            fn+=1\n    return tp, fp, fn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def filter_duplicates_and_threshols(row):\n    # Compute intersection over union\n    boxes,scores,labels = row.box_predictions,row.score_list,row.label_list\n    filtered_boxes = []\n    filtered_scores = []\n    filtered_labels = []\n    for box,score,label in zip(boxes,scores,labels):\n        # Assume score_list is sorted\n        if score> Threshold:\n            add_box = True\n            for i,ref_box in enumerate(filtered_boxes):\n                if iou(box,ref_box)>0.5:\n                    add_box = False\n                    if label==2:filtered_labels[i]=2\n\n            if add_box: \n                filtered_boxes.append(box)\n                filtered_scores.append(score)\n                filtered_labels.append(label)\n    row.box_predictions,row.score_list,row.label_list = filtered_boxes,filtered_scores,filtered_labels\n    return row\n\ndef filter_duplicates_and_low_scores(row):\n    # Compute intersection over union\n    boxes,scores,labels = row.box_predictions,row.score_list,row.label_list\n    ordered_sample_predictions = sorted(list(zip(boxes,scores,labels)), key = lambda x:x[1],\n                                        reverse = True)\n    filtered_boxes = []\n    filtered_scores = []\n    filtered_labels = []\n    for box,score,label in ordered_sample_predictions:\n        \n        if score>Threshold:\n            # Assume score_list is sorted\n            add_box = True\n\n            for i,ref_box in enumerate(filtered_boxes):\n                if iou(box,ref_box)>0.5:\n                    add_box = False\n                    if label==2:filtered_labels[i]=2\n\n            if add_box: \n                filtered_boxes.append(box)\n                filtered_scores.append(score)\n                filtered_labels.append(label)\n    row.box_predictions,row.score_list,row.label_list = filtered_boxes,filtered_scores,filtered_labels\n    return row","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"video_labels = pd.read_csv('off.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"video_labels = video_labels.sort_values(by=['video','frame']).reset_index(drop=True)\nvideo_labels = video_labels[video_labels.frame>0]\nvideo_labels['image'] = video_labels['video'].str.replace('.mp4', '') + '_' + video_labels['frame'].astype(str) + '.png'\nvideo_labels['right'] = video_labels['left'] + video_labels['width']\nvideo_labels['bottom'] = video_labels['top'] + video_labels['height']\nvideo_labels['label_list'] = video_labels['labels'].apply(lambda x:[x])\nvideo_labels['bbox'] = pd.Series(video_labels[['left','top','right','bottom']].values.tolist(), index=video_labels.index)\nvideo_labels['box_predictions'] = video_labels['bbox'].apply(lambda x:[x])\nvideo_labels['score_list'] = video_labels['scores'].apply(lambda x:[x])\nbase_cols = ['video','image','gameKey','frame']\ntarget_cols = ['box_predictions','label_list','score_list']\npredictions = video_labels[base_cols+target_cols].groupby(base_cols).sum().reset_index()\npredictions = predictions.apply(filter_duplicates_and_low_scores,axis=1)\npredictions['n_impact'] = predictions.label_list.apply(lambda x:(np.array(x)==2).sum())\npredictions.groupby('video').n_impact.sum().describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\ndef iou(bbox1, bbox2):\n    bbox1 = [float(x) for x in bbox1]\n    bbox2 = [float(x) for x in bbox2]\n\n    (x0_1, y0_1, x1_1, y1_1) = bbox1\n    (x0_2, y0_2, x1_2, y1_2) = bbox2\n\n    # get the overlap rectangle\n    overlap_x0 = max(x0_1, x0_2)\n    overlap_y0 = max(y0_1, y0_2)\n    overlap_x1 = min(x1_1, x1_2)\n    overlap_y1 = min(y1_1, y1_2)\n\n    # check if there is an overlap\n    if overlap_x1 - overlap_x0 <= 0 or overlap_y1 - overlap_y0 <= 0:\n            return 0\n\n    # if yes, calculate the ratio of the overlap to each ROI size and the unified size\n    size_1 = (x1_1 - x0_1) * (y1_1 - y0_1)\n    size_2 = (x1_2 - x0_2) * (y1_2 - y0_2)\n    size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0)\n    size_union = size_1 + size_2 - size_intersection\n\n    return size_intersection / size_union\n\ndef match_boxes(bbox_set1,bbox_set2,diagonal=True,iou_threshold=0.35):\n    # Compute intersection over union\n    all_pairs = {}\n    relation = [None for x in bbox_set2]\n    done1 = {}\n    done2 = {}\n    for i,bbox1 in enumerate(bbox_set1):\n        for j,bbox2 in enumerate(bbox_set2):\n            if i!=j or diagonal:\n                all_pairs[(i,j)] = iou(bbox1,bbox2)\n    for (index1,index2),iou_score in sorted(all_pairs.items(), key=lambda item: item[1],reverse=True):\n        if iou_score<iou_threshold: return relation\n        if (index1 not in done1) and (index2 not in done2):\n            relation[index2] = index1\n            done1[index1] = 1\n            done2[index2] = 1\n    return relation\n\ndef map_classes(row):\n    mapping = {}\n    for box,pred in zip(row.box_predictions,row.label_list):\n        mapping[tuple(box)] = pred\n    return mapping\n\ndef assign_players(df,video,LOOK_BACKWARD_LIMIT=LOOK_BACKWARD_LIMIT,IOU_THRESHOLD=IOU_THRESHOLD):\n    video_df = df[df.video==video].sort_values(by='frame').set_index('frame')\n    row_to_class_map = video_df['row_to_class_map']\n    row_to_class_map.apply(lambda x:x.update({():-1}))\n    player_to_bbox = pd.DataFrame()\n    bbox_to_player = {k+1:{} for k in range(video_df.shape[0])}\n    N_PLAYERS = 0\n    # Initial Player Prediction\n    bboxes = video_df.loc[1,'box_predictions']\n    for player_id,box in enumerate(bboxes):\n        player_to_bbox[player_id] = None\n        player_to_bbox.at[1,player_id] = box\n        bbox_to_player[1][tuple(box)] = player_id\n        N_PLAYERS += 1\n        \n\n    # Subsequent Player Prediction\n    for frame in range(2,video_df.shape[0]+1):\n        player_to_bbox.loc[frame] = None\n        bboxes = video_df.loc[frame,'box_predictions']\n        assigned = {}\n        look_backward = 1\n        while(look_backward<min(frame,LOOK_BACKWARD_LIMIT+1) and len(bboxes)>0):\n            ref_bbox = []\n            for box in video_df.loc[frame-look_backward,'box_predictions']:\n                if bbox_to_player[frame-look_backward][tuple(box)] not in assigned:\n                    ref_bbox.append(box)\n            relation = match_boxes(ref_bbox,bboxes,iou_threshold=IOU_THRESHOLD)\n            left_out_bboxes = []\n            # Assign Players\n            for index,ref_index in enumerate(relation):\n                if ref_index is not None:\n                    # Read Player ID and box\n                    player_id = bbox_to_player[frame-look_backward][tuple(ref_bbox[ref_index])]\n                    box = bboxes[index]\n                    # Write Player ID\n                    player_to_bbox.at[frame,player_id] = box\n                    bbox_to_player[frame][tuple(box)] = player_id\n                    assigned[player_id] = True\n                else:\n                    left_out_bboxes.append(bboxes[index])\n\n            bboxes = left_out_bboxes\n            look_backward +=1\n        # New Player Addition\n        for box in bboxes:\n            player_id = N_PLAYERS\n            player_to_bbox[player_id] = None\n            player_to_bbox.at[frame,player_id] = box\n            bbox_to_player[frame][tuple(box)] = player_id\n            N_PLAYERS += 1\n            \n    # Map Classes to Players\n    player_to_class_map = player_to_bbox.apply(lambda row: row.fillna('').apply(tuple).apply(\n            lambda x:row_to_class_map.loc[row.name][x]),axis=1)\n    \n    return player_to_bbox,player_to_class_map\n\ndef filter_impacts(player_series):\n    impacted_player_series = pd.DataFrame(player_series.loc[player_series==2])\n    impacted_player_series['indices'] = impacted_player_series.index\n    start_indices = impacted_player_series[impacted_player_series.indices.diff(1).fillna(99).abs()>9].indices.tolist()\n    end_indices = impacted_player_series[impacted_player_series.indices.diff(-1).fillna(99).abs()>9].indices.tolist()\n    output_series = player_series.copy(deep=True)\n    output_series[player_series>0] = 1\n    output_series[player_series<0] = -1\n    for start_index,end_index in zip(start_indices,end_indices):\n        difference = end_index-start_index+1\n        n_medians = math.ceil(difference/9)\n        difference_split = difference/(n_medians+1)\n        centres = [int(start_index+(i+1)*difference_split) for i in range(n_medians)]\n        output_series.loc[centres] = 2\n    return output_series","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame(columns=['gameKey','playID','view','video','frame','left','width','top','height'])\npredictions['row_to_class_map'] = predictions.apply(map_classes,axis=1)\nentries = 0\nfor video in predictions.video.unique():\n    # Assign Players\n    assigned_players_boxes,assigned_players_classes = assign_players(predictions,video)\n    keep_columns = assigned_players_classes.columns[(assigned_players_classes!=-1).sum()>keep_columns_threshold]\n#     break\n    assigned_players_boxes = assigned_players_boxes[keep_columns]\n    assigned_players_classes = assigned_players_classes[keep_columns]\n    print(assigned_players_boxes.shape)\n    # Filter interframe bbox\n    assigned_players_classes_filtered = assigned_players_classes.apply(filter_impacts)\n    n_filtered = (assigned_players_classes_filtered==2).sum().sum()\n    n_classes = (assigned_players_classes==2).sum().sum()\n    print(video,n_filtered,n_classes,(~assigned_players_boxes.isna()).sum().sum())\n    \n    # Only fill if frame gap between labelled boxes<10\n    all_bboxes = assigned_players_boxes.fillna(method='ffill').values.reshape(-1)\n    \n    \n    all_classes = assigned_players_classes_filtered.values.reshape(-1)\n    all_frames = assigned_players_boxes.apply(lambda x: pd.DataFrame(x).apply(\n        lambda y: y.name, axis=1)).values.reshape(-1)\n    bboxes_with_impact = all_bboxes[np.where(all_classes==2)]\n    frames_with_impact = all_frames[np.where(all_classes==2)]\n    \n    # Write to dataframe\n    gameKey,playID,view,_ = video.replace('.','_').split('_')\n    for frame,box in zip(frames_with_impact,bboxes_with_impact):\n        submission_df.loc[entries,'gameKey'] = gameKey\n        submission_df.loc[entries,'playID'] = playID\n        submission_df.loc[entries,'view'] = view\n        submission_df.loc[entries,'video'] = video\n        submission_df.loc[entries,'frame'] = frame\n        submission_df.loc[entries,'left'] = box[0]\n        submission_df.loc[entries,'top'] = box[1]\n        submission_df.loc[entries,'width'] = box[2]-box[0]\n        submission_df.loc[entries,'height'] = box[3]-box[1]\n        entries += 1\nsubmission_df = submission_df[(submission_df.frame<FRAME_THRESHOLD) & (submission_df.frame>20)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df1 = submission_df.copy(deep=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 42\nFOLDS = 5\nFOLD = 2\nEFF_DET = 5 \nThreshold = 0.65\nLOOK_BACKWARD_LIMIT = 10\nIOU_THRESHOLD = 0.4\nIMAGE_SIZE = 512\nNUM_CLASSES = 2\nBATCH_SIZE = 16\nFRAME_THRESHOLD = 240\nkeep_columns_threshold = 20\n\nvideo_labels = pd.read_csv('off.csv')\nvideo_labels = video_labels.sort_values(by=['video','frame']).reset_index(drop=True)\nvideo_labels = video_labels[video_labels.frame>0]\nvideo_labels['image'] = video_labels['video'].str.replace('.mp4', '') + '_' + video_labels['frame'].astype(str) + '.png'\nvideo_labels['right'] = video_labels['left'] + video_labels['width']\nvideo_labels['bottom'] = video_labels['top'] + video_labels['height']\nvideo_labels['label_list'] = video_labels['labels'].apply(lambda x:[x])\nvideo_labels['bbox'] = pd.Series(video_labels[['left','top','right','bottom']].values.tolist(), index=video_labels.index)\nvideo_labels['box_predictions'] = video_labels['bbox'].apply(lambda x:[x])\nvideo_labels['score_list'] = video_labels['scores'].apply(lambda x:[x])\nbase_cols = ['video','image','gameKey','frame']\ntarget_cols = ['box_predictions','label_list','score_list']\npredictions = video_labels[base_cols+target_cols].groupby(base_cols).sum().reset_index()\npredictions = predictions.apply(filter_duplicates_and_low_scores,axis=1)\npredictions['n_impact'] = predictions.label_list.apply(lambda x:(np.array(x)==2).sum())\npredictions.groupby('video').n_impact.sum().describe()\n\nsubmission_df = pd.DataFrame(columns=['gameKey','playID','view','video','frame','left','width','top','height'])\npredictions['row_to_class_map'] = predictions.apply(map_classes,axis=1)\nentries = 0\nfor video in predictions.video.unique():\n    # Assign Players\n    assigned_players_boxes,assigned_players_classes = assign_players(predictions,video)\n    keep_columns = assigned_players_classes.columns[(assigned_players_classes!=-1).sum()>keep_columns_threshold]\n#     break\n    assigned_players_boxes = assigned_players_boxes[keep_columns]\n    assigned_players_classes = assigned_players_classes[keep_columns]\n    print(assigned_players_boxes.shape)\n    # Filter interframe bbox\n    assigned_players_classes_filtered = assigned_players_classes.apply(filter_impacts)\n    n_filtered = (assigned_players_classes_filtered==2).sum().sum()\n    n_classes = (assigned_players_classes==2).sum().sum()\n    print(video,n_filtered,n_classes,(~assigned_players_boxes.isna()).sum().sum())\n    \n    # Only fill if frame gap between labelled boxes<10\n    all_bboxes = assigned_players_boxes.fillna(method='ffill').values.reshape(-1)\n    \n    \n    all_classes = assigned_players_classes_filtered.values.reshape(-1)\n    all_frames = assigned_players_boxes.apply(lambda x: pd.DataFrame(x).apply(\n        lambda y: y.name, axis=1)).values.reshape(-1)\n    bboxes_with_impact = all_bboxes[np.where(all_classes==2)]\n    frames_with_impact = all_frames[np.where(all_classes==2)]\n    \n    # Write to dataframe\n    gameKey,playID,view,_ = video.replace('.','_').split('_')\n    for frame,box in zip(frames_with_impact,bboxes_with_impact):\n        submission_df.loc[entries,'gameKey'] = gameKey\n        submission_df.loc[entries,'playID'] = playID\n        submission_df.loc[entries,'view'] = view\n        submission_df.loc[entries,'video'] = video\n        submission_df.loc[entries,'frame'] = frame\n        submission_df.loc[entries,'left'] = box[0]\n        submission_df.loc[entries,'top'] = box[1]\n        submission_df.loc[entries,'width'] = box[2]-box[0]\n        submission_df.loc[entries,'height'] = box[3]-box[1]\n        entries += 1\nsubmission_df = submission_df[(submission_df.frame<FRAME_THRESHOLD) & (submission_df.frame>120)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df2 = submission_df.copy(deep=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.concat([submission_df1,submission_df2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv * /tmp/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nflimpact\nenv = nflimpact.make_env()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_PRIVATE:\n    env.predict(submission_df) # df is a pandas dataframe of your entire submission file\nelse:\n    sub = pd.read_csv('../input/nfl-impact-detection/sample_submission.csv')\n    env.predict(sub)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}